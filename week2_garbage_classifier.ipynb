{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Importing necessary libraries for the project\n",
        "import numpy as np  # Importing NumPy for numerical operations and array manipulations\n",
        "import matplotlib.pyplot as plt  # Importing Matplotlib for plotting graphs and visualizations\n",
        "import seaborn as sns  # Importing Seaborn for statistical data visualization, built on top of Matplotlib\n",
        "import tensorflow as tf  # Importing TensorFlow for building and training machine learning models\n",
        "from tensorflow import keras  # Importing Keras, a high-level API for TensorFlow, to simplify model building\n",
        "from tensorflow.keras import Layer  # Importing Layer class for creating custom layers in Keras\n",
        "from tensorflow.keras.models import Sequential  # Importing Sequential model for building neural networks layer-by-layer\n",
        "from tensorflow.keras.layers import Rescaling , GlobalAveragePooling2D\n",
        "from tensorflow.keras import layers, optimizers, callbacks  # Importing various modules for layers, optimizers, and callbacks in Keras\n",
        "from sklearn.utils.class_weight import compute_class_weight  # Importing function to compute class weights for imbalanced datasets\n",
        "from tensorflow.keras.applications import EfficientNetV2B2  # Importing EfficientNetV2S model for transfer learning\n",
        "from sklearn.metrics import confusion_matrix, classification_report  # Importing functions to evaluate model performance\n",
        "import gradio as gr\n"
      ],
      "metadata": {
        "id": "QE9mI_vN3Voh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()  # Upload your dataset zip (e.g., trash_dataset.zip)\n"
      ],
      "metadata": {
        "id": "1FrD8XvKDK5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip archive\\ \\(3\\).zip -d dataset/\n"
      ],
      "metadata": {
        "id": "Rl0xCXrRWuxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    \"dataset/TrashType_Image_Dataset\",   # ✅ updated path\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=123,\n",
        "    image_size=(180, 180),\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    \"dataset/TrashType_Image_Dataset\",   # ✅ updated path\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=123,\n",
        "    image_size=(180, 180),\n",
        "    batch_size=32\n",
        ")\n"
      ],
      "metadata": {
        "id": "5YTydRIb56VG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_class=val_ds.class_names\n",
        "print(val_class)"
      ],
      "metadata": {
        "id": "iFMhVAVmY2VO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_batches=tf.data.experimental.cardinality(val_ds)\n",
        "print(val_batches)\n",
        "\n"
      ],
      "metadata": {
        "id": "bmFZKAhw4fTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_ds=val_ds.take(val_batches //2)\n",
        "val_dat=val_ds.skip(val_batches //2)\n",
        "test_ds_eval=test_ds.cache().prefetch(tf.data.AUTOTUNE)\n",
        "print(train_ds.class_names)\n",
        "print(val_class)\n",
        "print(len(train_ds.class_names))\n"
      ],
      "metadata": {
        "id": "ksRVsDr4ZnZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10, 10))\n",
        "for images, labels in train_ds.take(1):\n",
        "    for i in range(12):\n",
        "        ax = plt.subplot(4, 3, i + 1)\n",
        "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "        plt.title(train_ds.class_names[labels[i]])\n",
        "        plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "srgamz53bLCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_distribution(dataset, class_names):\n",
        "    total = 0\n",
        "    counts = {name: 0 for name in class_names}\n",
        "\n",
        "    for _, labels in dataset:\n",
        "        for label in labels.numpy():\n",
        "            class_name = class_names[label]\n",
        "            counts[class_name] += 1\n",
        "            total += 1\n",
        "\n",
        "    for k in counts:\n",
        "        counts[k] = round((counts[k] / total) * 100, 2)  # Convert to percentage\n",
        "    return counts"
      ],
      "metadata": {
        "id": "PQtdkBhKeaTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def simple_bar_plot(dist, title):\n",
        "    plt.bar(dist.keys(), dist.values(), color='cornflowerblue')\n",
        "    plt.title(title)\n",
        "    plt.ylabel('Percentage (%)')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.ylim(0, 100)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "pev1xJZWeaNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9a488f30"
      },
      "source": [
        "# Ensure datasets are defined\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    \"dataset/TrashType_Image_Dataset\",\n",
        "    validation_split=0.2,\n",
        "    subset=\"training\",\n",
        "    seed=123,\n",
        "    image_size=(180, 180),\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    \"dataset/TrashType_Image_Dataset\",\n",
        "    validation_split=0.2,\n",
        "    subset=\"validation\",\n",
        "    seed=123,\n",
        "    image_size=(180, 180),\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "# Recalculate val_batches and test_ds after re-creating val_ds\n",
        "val_batches = tf.data.experimental.cardinality(val_ds)\n",
        "test_ds_split = val_ds.take(val_batches // 2)\n",
        "val_dat = val_ds.skip(val_batches // 2)\n",
        "\n",
        "class_names=train_ds.class_names\n",
        "train_dist=count_distribution(train_ds,class_names)\n",
        "val_dist=count_distribution(val_ds,class_names)\n",
        "test_dist = count_distribution(test_ds_split, class_names)\n",
        "\n",
        "overall_dist={}\n",
        "for k in class_names:\n",
        "  overall_dist[k] = round((train_dist[k] + val_dist[k]) / 2, 2)\n",
        "\n",
        "print(\"Training Distribution:\", train_dist)\n",
        "print(\"Validation Distribution:\", val_dist)\n",
        "print(\"Test Distribution:\", test_dist)\n",
        "print(\"Overall Distribution:\", overall_dist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_counts={i:0 for i in range(len(class_names))}\n",
        "all_labels=[]\n",
        "for images,labels in train_ds:\n",
        "  for label in labels.numpy():\n",
        "    class_counts[label]+=1\n",
        "    all_labels.append(label)\n",
        "\n",
        "class_weights_array=compute_class_weight(class_weight='balanced',classes=np.arange(len(class_names)),y=all_labels)\n",
        "class_weights={i: w for i,w in enumerate(class_weights_array)}\n",
        "print(class_counts)\n",
        "print(class_weights)"
      ],
      "metadata": {
        "id": "mw2vldtTKbCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_augmentation=Sequential([layers.RandomFlip(\"horizontal\"),layers.RandomRotation(0.1),\n",
        "    layers.RandomZoom(0.1),\n",
        "    layers.RandomContrast(0.1),])\n"
      ],
      "metadata": {
        "id": "QMa2YaxUKa_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model=EfficientNetV2B2(include_top=False,weights='imagenet',input_shape=(180,180,3),include_preprocessing=True)\n",
        "base_model.trainable = True\n",
        "for layer in base_model.layers[:100]:  # You can adjust this number\n",
        "    layer.trainable = False\n"
      ],
      "metadata": {
        "id": "aIqs8YzgKa8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=Sequential([layers.Input((180,180,3)),data_augmentation,base_model,GlobalAveragePooling2D(),layers.Dropout(0.3),layers.Dense(len(class_names),activation='softmax')])\n",
        "model.compile(optimizer=optimizers.Adam(learning_rate=1e-4),loss='sparse_categorical_crossentropy',metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "sy6uBnhFKa7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "early = tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=3,restore_best_weights=True)\n",
        "epochs=3\n",
        "history=model.fit(train_ds,validation_data=val_ds,epochs=epochs,class_weight=class_weights,batch_size=32,callbacks=[early])\n",
        "model.summary()\n",
        "base_model.summary()"
      ],
      "metadata": {
        "id": "_5so4riDKajh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"week2_model.keras\")"
      ],
      "metadata": {
        "id": "BcUeb5uNRq5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uSE3V2JtRq26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yftAFcDRRqz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bFEip4mXRqfP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}